{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the correlations between the various assay datasets to combine them into a \"tmprss2 equivalent\" dataset.  The algorithm goes like this:\n",
    "For each non-TMPRSS2 target:\n",
    "1. Find the set S of chemicals which were assayed against both TMPRSS2 and the target protein\n",
    "2. Manually remove outliers from S\n",
    "3. Perform a linear regression on S, x=TMPRSS2 activites, y=off-target activities.  Outliers are manually removed by inspection.  Collect the slope, and calculate a spearman_r correlation.\n",
    "4. If spearman_r is high (>.70), use the calculated slope to rescale the target's activity values.\n",
    "5. Construct combined dataset\n",
    "\n",
    "Combined Dataset Columns:\n",
    "\n",
    "* **acvalue_target**: The activity as measured in the original assay, i.e. activity on \"target\"\n",
    "* **active_target**: Whether acvalue_target is less than some threshold, defined in data processing notebooks (currently 50)\n",
    "* **target**: The protein the assay was conducted on (e.g. ST14, TMPRSS2)\n",
    "* **tmprss2_vs_target_spearman_r**: the spearman_r correlation from algorithm step 3\n",
    "* **acvalue_scaled_to_tmprss2**: rescaled activity value from algorithm step 4\n",
    "* **cid**: cid of compound\n",
    "* **smiles**: smiles of compound\n",
    "\n",
    "\n",
    "Note 1: spearman-r is used over pearson-r because our data has a skewed distribution.  See this article (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3576830/)\n",
    "\n",
    "Note 2: the reason I only adjust by slope and not also by intercept is because adjusting by intercept could result in negative activities, which don't make sense.  Fixing intercept at 0 would solve this, but does not seem to be a reasonable assumption.  A near-zero activity on an off-target protein doesn't necessarily mean a near-zero activity on TMPRSS2.  In practice, the correlated off-targets have intercepts near zero, so this is not worth worrying about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "from scipy import stats\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.max_colwidth', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect datasets\n",
    "TMPRSS2 = pd.read_csv('../dumps/TMPRSS2_processed.csv')\n",
    "\n",
    "off_targets = {'KLKB1':     pd.read_csv('../dumps/KLKB1_processed.csv'),\n",
    "               'ST14':      pd.read_csv('../dumps/ST14_processed.csv'),\n",
    "               'TMPRSS6':   pd.read_csv('../dumps/TMPRSS6_processed.csv'),\n",
    "               'TMPRSS11D': pd.read_csv('../dumps/TMPRSS11D_processed.csv')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate overlaps\n",
    "overlaps = {}\n",
    "for name, off_target in off_targets.items():\n",
    "    print(len(off_target))\n",
    "    overlaps[name] = pd.merge(TMPRSS2, off_target, on=['cid', 'smiles'])\n",
    "    # make sure 'cid' and 'smiles' are one-to-one\n",
    "    assert len(overlaps[name]) == len(pd.merge(TMPRSS2, off_target, on=['cid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize the overlap correlations\n",
    "def visualize_overlaps(overlaps, square=False):\n",
    "    for name, overlap in overlaps.items():\n",
    "        plt.figure()\n",
    "        x = overlap['acvalue_x']\n",
    "        y = overlap['acvalue_y']\n",
    "        try:\n",
    "            sns.regplot(x, y, label=f'spearman_r: {spearmanr(x, y)[0]: .2f}')\n",
    "        except ValueError:\n",
    "            sns.regplot(x, y, label=f'spearman_r: undefined')\n",
    "        plt.legend()\n",
    "        plt.xlabel('TMPRSS2 Activity Value')\n",
    "        plt.ylabel(f'{name} Activity Value')\n",
    "        plt.title(f'Correlation with {name}')\n",
    "        if square:\n",
    "            plt.axis('square')\n",
    "visualize_overlaps(overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually drop outliers by inspection\n",
    "cleaned_overlaps = {}\n",
    "cleaned_overlaps['ST14'] = overlaps['ST14'][overlaps['ST14']['acvalue_x'] < .2]\n",
    "cleaned_overlaps['TMPRSS6'] = overlaps['TMPRSS6'][overlaps['TMPRSS6']['acvalue_x'] < .2]\n",
    "cleaned_overlaps['TMPRSS11D'] = overlaps['TMPRSS11D'][overlaps['TMPRSS11D']['acvalue_x'] < 7]\n",
    "\n",
    "visualize_overlaps(cleaned_overlaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary of the off-targets:\n",
    "1. KLKB1 has insufficient data to determine correlation\n",
    "2. ST14 has a strong correlation with TMPRSS2\n",
    "3. TMPRSS6 has a strong correlation with TMPRSS2\n",
    "4. TMPRSS11D has no correlation with TMPRSS2\n",
    "\n",
    "We will thus only bother with scaling ST14 and TMPRSS6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "print('Before Scaling')\n",
    "visualize_overlaps({name: cleaned_overlaps[name] for name in ['ST14', 'TMPRSS6']}, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_slopes = {'KLKB1': np.nan, 'TMPRSS11D': np.nan}\n",
    "scaled_overlaps = {}\n",
    "for name in ['ST14', 'TMPRSS6']:\n",
    "    overlap = cleaned_overlaps[name].copy()\n",
    "    x = overlap['acvalue_x']\n",
    "    y = overlap['acvalue_y']\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "    regression_slopes[name] = slope\n",
    "    \n",
    "    scaled_overlaps[name] = overlap\n",
    "    scaled_overlaps[name]['acvalue_y'] /= slope\n",
    "print('After Scaling')\n",
    "visualize_overlaps(scaled_overlaps, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate spearman_rs\n",
    "spearman_rs = {}\n",
    "cleaned_overlaps\n",
    "for name, overlap in cleaned_overlaps.items():\n",
    "    spearman_rs[name] = spearmanr(overlap['acvalue_x'], overlap['acvalue_y'])[0]\n",
    "spearman_rs['KLKB1'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Finally, construct the combined dataset\n",
    "orig_datasets = off_targets\n",
    "orig_datasets['TMPRSS2'] = TMPRSS2\n",
    "\n",
    "spearman_rs['TMPRSS2'] = 1\n",
    "regression_slopes['TMPRSS2'] = 1\n",
    "\n",
    "reformatted_datasets = []\n",
    "for target in ['TMPRSS2', 'KLKB1', 'ST14', 'TMPRSS6', 'TMPRSS11D']:\n",
    "    ds = orig_datasets[target].copy()\n",
    "    ds = ds.rename(columns={'acvalue': 'acvalue_target', 'activity': 'activity_target'})\n",
    "    ds['target'] = target\n",
    "    ds['acvalue_scaled_to_tmprss2'] = ds['acvalue_target'] / regression_slopes[target]\n",
    "    ds['tmprss2_vs_target_spearman_r'] = spearman_rs[target]\n",
    "    reformatted_datasets.append(ds)\n",
    "    print(target)\n",
    "    print(ds.head())\n",
    "\n",
    "combined_dataset = pd.concat(reformatted_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder columns\n",
    "combined_dataset = combined_dataset[['acvalue_target', 'activity_target', 'target', 'tmprss2_vs_target_spearman_r', 'acvalue_scaled_to_tmprss2', 'cid', 'smiles']]\n",
    "combined_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Morgan Fingerprints for all the SMILES codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import rdMolDescriptors, MolFromSmiles\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprint_function = partial(rdMolDescriptors.GetMorganFingerprintAsBitVect, \n",
    "                              radius=2, useChirality=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fingerprint_to_np(fp):\n",
    "    bit_string = fp.ToBitString()\n",
    "    return np.array([int(char) for char in bit_string], dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset['morgan_fingerprint'] = combined_dataset.apply(lambda row: fingerprint_to_np(fingerprint_function(MolFromSmiles(row.smiles))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file to disk by pickling.  Could use csv, but turns fingerprint into a string.\n",
    "combined_dataset.to_pickle('../dumps/combined_dataset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: how to recover dataframe from saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loaded = pd.read_pickle('../dumps/combined_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loaded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# demonstration that fingerprint is loaded correctly\n",
    "print(\"Fingerprint type:\", type(df_loaded.morgan_fingerprint.iloc[0]))\n",
    "print(\"Fingerprint length:\", df_loaded.morgan_fingerprint.iloc[0].shape)\n",
    "print(\"First fingerprint equal:\", np.all(df_loaded.morgan_fingerprint.iloc[0] == combined_dataset.morgan_fingerprint.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

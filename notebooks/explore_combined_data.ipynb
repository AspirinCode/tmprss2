{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand the Combined Data\n",
    "Let's understand what we have to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"../processed_data/combined_dataset.pkl\")\n",
    "data.sample(frac=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulk Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data.acvalue_target,\n",
    "         bins=np.logspace(np.log10(data.acvalue_target.min()), np.log10(data.acvalue_target.max()), 50),\n",
    "         label='All Labels')\n",
    "plt.gca().set_xscale(\"log\")\n",
    "plt.xlabel('activity on target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminated by target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('target').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = data.groupby('target').acvalue_target.apply(list).to_dict()\n",
    "fig, axes = plt.subplots(len(targets), 1, sharey=True, sharex=True)\n",
    "fig.set_figheight(10)\n",
    "axes = iter(axes)\n",
    "colors = iter(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])\n",
    "for name, acvalues in targets.items():\n",
    "    ax = next(axes)\n",
    "    ax.hist(acvalues,\n",
    "            bins=np.logspace(np.log10(data.acvalue_target.min()), np.log10(data.acvalue_target.max()), 50),\n",
    "            color=next(colors))\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylabel(name)\n",
    "ax.set_xlabel('activity')\n",
    "plt.suptitle('Activity Values by Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ST14, TMPRSS11D, and TMPRSS2 appear to have similar activity distributions.\n",
    "\n",
    "KLKB1 is somewhat flat and skewed right, but also similar.\n",
    "\n",
    "TMPRSS6 is notably skewed right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Activity Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = data.groupby('target').acvalue_scaled_to_tmprss2.apply(list).to_dict()\n",
    "fig, axes = plt.subplots(len(targets), 1, sharey=True, sharex=True)\n",
    "fig.set_figheight(10)\n",
    "axes = iter(axes)\n",
    "colors = iter(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])\n",
    "for name, acvalues in targets.items():\n",
    "    ax = next(axes)\n",
    "    ax.hist(acvalues,\n",
    "            bins=np.logspace(np.log10(data.acvalue_target.min()), np.log10(data.acvalue_target.max()), 50),\n",
    "            color=next(colors))\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylabel(name)\n",
    "    ax.axvline(np.median(acvalues), color='b', label='median')\n",
    "    ax.axvline(np.mean(acvalues), color='g', label='mean')\n",
    "ax.set_xlabel('activity')\n",
    "ax.legend()\n",
    "plt.suptitle('Activity values scaled to TMPRSS2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the linear regression and scaling is just a fancy way to put the distribution medians inline.  It does have more scientific merit, though, since the scaling only takes into account values from compounds which appear in both datasets.  Still, though, it suggests that if we would like to utilize all the data, including KLKB1 and ST14, it might be smart to scale the activities to match medians.  Definitely don't mean-scale the data though, since it appears to be mainly log-normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
